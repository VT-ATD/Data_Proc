{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd \n",
    "import re\n",
    "from pandas import Series, DataFrame\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "import gensim\n",
    "\n",
    "# directory = \"/home/sdbookhu/FullNews/\"\n",
    "\n",
    "directory = \"C:/Users/Shreya/Desktop/all un news/\"\n",
    "\n",
    "for files in glob.glob(directory+'*.csv'):\n",
    "\n",
    "    print(files[35:])\n",
    "\n",
    "    news_source = []\n",
    "    news_title = []\n",
    "    news_description = []\n",
    "    news_url = []\n",
    "    news_publish_time = []\n",
    "    news_content = []\n",
    "    news_full_content = []\n",
    "\n",
    "    #      save_directory = \"/home/shreyac/cleaned_news/\"\n",
    "\n",
    "    save_directory = \"C:/Users/Shreya/Desktop/all pro news/\"\n",
    "\n",
    "    name = files[35:] # 23\n",
    "\n",
    "    news_csv_file = pd.read_csv(files)\n",
    "\n",
    "    data_all = news_csv_file\n",
    "    data_url = data_all['url']\n",
    "    data_source = data_all['source_name']\n",
    "    data_publish_time = data_all['publishedAt']\n",
    "    data_title = data_all['title']\n",
    "    data_description = data_all['description']\n",
    "    data_content = data_all['content']\n",
    "    data_full_content = data_all['full_content']\n",
    "    \n",
    "#     data_full_content = data_all['full-content']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Use the script below to make changes to the CSV file and save as a different CSV file \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#     for i in range(0, len(data_all)):\n",
    "\n",
    "#         news_url.append(data_url[i])\n",
    "#         news_source.append(data_source[i])\n",
    "#         news_publish_time.append(data_publish_time[i])\n",
    "#         news_title.append(data_title[i])\n",
    "#         news_description.append(data_description[i])\n",
    "#         news_content.append(data_content[i])\n",
    "#         news_full_content.append(data_full_content[i])\n",
    "\n",
    "\n",
    "#     news_file_df = DataFrame({'url': news_url,\n",
    "#                     'source': news_source,\n",
    "#                     'published_at': news_publish_time,\n",
    "#                     'title': news_title,\n",
    "#                     'description': news_description,\n",
    "#                     'content': news_content,\n",
    "#                     'full_content': news_full_content})\n",
    "\n",
    "#     news_file_df = news_file_df[['url', 'source', 'published_at', 'title',\n",
    "#                                      'description', 'content', 'full_content']]\n",
    "\n",
    "# #     save_directory = save_directory + 'enriched_' + name \n",
    "\n",
    "#     save_directory = save_directory + name\n",
    "#     print(save_directory)\n",
    "\n",
    "#     export_csv = news_file_df.to_csv (save_directory, index = None, header=True)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- PRE-PROCESS ---------------------------------------------------------\n",
    "\n",
    "\n",
    "    data_text = data_all[['full_content', 'title']]\n",
    "#     print(len(data_text))\n",
    "    data_text = data_text.dropna() # Not all articles have \"full content\" available\n",
    "#     print(len(data_text))\n",
    "    data_text = data_text.drop_duplicates(subset=\"title\", keep = \"last\") # We have many repeating articles\n",
    "#     print(len(data_text))\n",
    "    \n",
    "    \n",
    "    documents = data_text[['full_content']]\n",
    "    \n",
    "    # Remove hyperlinks from content\n",
    "    link_remove = re.compile(r'http\\S+')                             \n",
    "    documents['full_content'].replace(to_replace= link_remove, value='', regex=True, inplace=True)\n",
    "    \n",
    "    remove_let_your_friends_know = re.compile(r'Let friends in your social network know what you are reading about .*? Please read the rules before joining the discussion.')\n",
    "    documents['full_content'].replace(to_replace= remove_let_your_friends_know, value='', regex=True, inplace=True)\n",
    "    \n",
    "    remove_last_for_more_coverage_1 = re.compile(r'___ For more .*? This material may not be published, broadcast, rewritten or redistributed.')\n",
    "    remove_last_for_more_coverage_2 = re.compile(r'___ For more .*? by Automated Insights,  using data from STATS LLC, ')\n",
    "    remove_last_for_more_coverage_3 = re.compile(r'For more AP.*? by Automated Insights,  using data from STATS LLC, ')\n",
    "    documents['full_content'].replace(to_replace= remove_last_for_more_coverage_1, value='', regex=True, inplace=True)\n",
    "    documents['full_content'].replace(to_replace= remove_last_for_more_coverage_2, value='', regex=True, inplace=True)\n",
    "    documents['full_content'].replace(to_replace= remove_last_for_more_coverage_3, value='', regex=True, inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "#     Pre-procesing function\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['know', 'what', 'would', 'going', 'like', 'getting', \n",
    "#                    'come', 'felt', 'whatever', 'that', 'come', 'always', \n",
    "#                    'also', 'shall', 'thing', 'good', 'maybe', 'thank'])\n",
    "\n",
    "\n",
    "    def preprocess_text(doc):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pre-processing using textblob: \n",
    "    tokenizing, converting to lower-case, and lemmatization based on POS tagging, \n",
    "    removing stop-words, and retaining tokens greater than length 2\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        blob = TextBlob(doc)\n",
    "        result = []\n",
    "        tag_dict = {\"J\": 'a', # Adjective\n",
    "                    \"N\": 'n', # Noun\n",
    "                    \"V\": 'v', # Verb\n",
    "                    \"R\": 'r'} #  Adverb\n",
    "   \n",
    "        for sent in blob.sentences:\n",
    "        \n",
    "            words_and_tags = [(w, tag_dict.get(pos[0])) for w, pos in sent.tags]    \n",
    "            lemmatized_list = [w.lemmatize(tag) for w, tag in words_and_tags]\n",
    "    #         stemmed_list = [w.stem() for w in sent.word]\n",
    "        \n",
    "            for token in lemmatized_list:\n",
    "                if token.lower() not in stop_words and len(token.lower()) > 2:\n",
    "                    result.append(token.lower())\n",
    "    \n",
    "#     print(\" \".join(result))\n",
    "#     print(\" \")\n",
    "        return result\n",
    "\n",
    "\n",
    "#     # Example\n",
    "    \n",
    "#     doc_sample = documents[documents.index == 2].values[0][0]\n",
    "\n",
    "#     print('original document: ')\n",
    "#     print(doc_sample)\n",
    "#     print('\\n\\n tokenized and lemmatized document: ')\n",
    "#     print(preprocess_text(doc_sample))\n",
    "\n",
    "#     processed_sample = preprocess_text(doc_sample)\n",
    "\n",
    "\n",
    "    # We will apply preprocessing to the whole dataframe\n",
    "    \n",
    "    import swifter # Makes applying to datframe as fast as vectorizing\n",
    "\n",
    "    processed_docs = documents['full_content'].swifter.apply(preprocess_text).to_frame('processed_text') \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    To get full \"dictionary\" -- for word frequencies\n",
    "    \"\"\"\n",
    "    \n",
    "    all_processed_docs_list = processed_docs.processed_text.to_list() # Converts all rows to one big list of lists\n",
    "    all_processed_docs_list = [item for sublist in all_processed_docs_list for item in sublist] # List of lists to one simple list\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Function for word frequencies\n",
    "    \n",
    "    def get_frequency(processed_text_list): \n",
    "        \n",
    "        \"\"\"\n",
    "        An NLTK function\n",
    "        Gets frequency distribution of all words in a tokenized list\n",
    "        We also sort these frequencies in descending order\n",
    "        \"\"\"\n",
    "    \n",
    "        word_frequency = FreqDist(word for word in processed_text_list)\n",
    "\n",
    "        sorted_counts = sorted(word_frequency.items() , key = lambda x: x[1] ,\n",
    "                                   reverse = True)\n",
    "\n",
    "        return sorted_counts\n",
    "    \n",
    "    \n",
    "    all_docs_frequency = get_frequency(all_processed_docs_list) \n",
    "    \n",
    "    \n",
    "#     for i in all_docs_frequency:\n",
    "#         print(i)\n",
    "\n",
    "\n",
    "#     Gensim's in-built dictionary\n",
    "\n",
    "    text_dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    \n",
    "    \"\"\"\n",
    "    gensim has its own high and low pass filters as shown below.\n",
    "    However, we are unable to see exactly which words were removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Include words in dictionary that appear greater than 5 times - Low pass\n",
    "    # but less than 0.5 proportion of the frequency of all the words in all of the articles - High pass\n",
    "\n",
    "    text_dictionary.filter_extremes(no_below = 5, no_above=0.5) \n",
    "    print(len(text_dictionary))\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    All the words have been mapped to an \"ID\" using gensim.corpora.Dictionary\n",
    "    Now, within each individual \"document\" (news article), we can get the corresponding word counts.\n",
    "    Unfortunately, gensim only allows for getting document-level word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    main_corpus = [text_dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    \n",
    "    \n",
    "    # TF-IDF on the bag of words corpus\n",
    "\n",
    "    from gensim import models\n",
    "\n",
    "    tfidf = models.TfidfModel(main_corpus)\n",
    "    tfidf_main_corpus = tfidf[main_corpus]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Now, do LDA\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
