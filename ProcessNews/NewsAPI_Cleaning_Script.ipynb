{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shreya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tqdm\\autonotebook\\__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd \n",
    "import re\n",
    "from pandas import Series, DataFrame\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "import gensim\n",
    "from gensim import models # For TF-IDF\n",
    "import swifter # Makes applying to datframe as fast as vectorizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FUNCTIONS ---------------------------------------------------------\n",
    "\n",
    "# Pre-procesing function\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['chars', 'char']) # Add from blacklist\n",
    "\n",
    "def preprocess_text(doc):\n",
    "    \n",
    "    \"\"\"\n",
    "    pre-processing using textblob: \n",
    "    tokenizing, converting to lower-case, and lemmatization based on POS tagging, \n",
    "    removing stop-words, and retaining tokens greater than length 2\n",
    "    \"\"\"\n",
    "\n",
    "    blob = TextBlob(doc)\n",
    "    result = []\n",
    "    tag_dict = {\"J\": 'a', # Adjective\n",
    "                \"N\": 'n', # Noun\n",
    "                \"V\": 'v', # Verb\n",
    "                \"R\": 'r'} #  Adverb\n",
    "\n",
    "    for sent in blob.sentences:\n",
    "\n",
    "        words_and_tags = [(w, tag_dict.get(pos[0])) for w, pos in sent.tags]    \n",
    "        lemmatized_list = [w.lemmatize(tag) for w, tag in words_and_tags]\n",
    "#         stemmed_list = [w.stem() for w in sent.word]\n",
    "\n",
    "        for token in lemmatized_list:\n",
    "            if token.lower() not in stop_words and len(token.lower()) > 2:\n",
    "                result.append(token.lower())\n",
    "\n",
    "#     print(\" \".join(result))\n",
    "#     print(\" \")\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Function for word frequencies\n",
    "\n",
    "def get_frequency(processed_text_list): \n",
    "\n",
    "    \"\"\"\n",
    "    An NLTK function\n",
    "    Gets frequency distribution of all words in a tokenized list\n",
    "    We also sort these frequencies in descending order\n",
    "    \"\"\"\n",
    "\n",
    "    word_frequency = FreqDist(word for word in processed_text_list)\n",
    "\n",
    "    sorted_counts = sorted(word_frequency.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    return sorted_counts\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AllContent2019-03-02.csv\n",
      "AllContent2019-03-03.csv\n",
      "AllContent2019-03-04.csv\n",
      "AllContent2019-03-05.csv\n",
      "AllContent2019-03-06.csv\n",
      "AllContent2019-03-07.csv\n",
      "AllContent2019-03-08.csv\n",
      "AllContent2019-03-09.csv\n",
      "AllContent2019-03-10.csv\n",
      "AllContent2019-03-11.csv\n",
      "AllContent2019-03-12.csv\n"
     ]
    }
   ],
   "source": [
    "# directory = \"C:/Users/Shreya/Desktop/Threat_detective/all un news/\"\n",
    "\n",
    "# for files in glob.glob(directory + '*.csv'):\n",
    "\n",
    "#     print(files[53:]) #23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AllContent2019-03-02.csv\n",
      "Total number of short articles is:  1596\n",
      "Total number of short articles after dropping blank ones:  1343\n",
      "Total number of unique short articles is:  1044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3408ca009d514fb5a67720a8821eb4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1044, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total length of short content dictionary before filtering is:  8565\n",
      "Total length of short content dictionary after filtering is:  1112\n",
      "Total number of long articles is:  1596\n",
      "Total number of long articles after dropping blank ones:  1312\n",
      "Total number of unique long articles is:  1004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6eb96b21041482cb0c3ea8265d21920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1004, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total length of long content dictionary before filtering is:  30289\n",
      "Total length of long content dictionary after filtering is:  5687\n",
      "AllContent2019-03-02.csv done! Next file:\n",
      "AllContent2019-03-03.csv\n",
      "Total number of short articles is:  1561\n",
      "Total number of short articles after dropping blank ones:  1340\n",
      "Total number of unique short articles is:  986\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d114e2652a487f9873f3fab23fa7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=986, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total length of short content dictionary before filtering is:  8259\n",
      "Total length of short content dictionary after filtering is:  1030\n",
      "Total number of long articles is:  1561\n",
      "Total number of long articles after dropping blank ones:  1329\n",
      "Total number of unique long articles is:  966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20312ccc176141d7833e8f16bb4e6df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=966, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total length of long content dictionary before filtering is:  28761\n",
      "Total length of long content dictionary after filtering is:  5509\n",
      "AllContent2019-03-03.csv done! Next file:\n",
      "AllContent2019-03-04.csv\n",
      "Total number of short articles is:  4894\n",
      "Total number of short articles after dropping blank ones:  3806\n",
      "Total number of unique short articles is:  1541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4559d013724678a30fdb0c8164bf30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1541, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total length of short content dictionary before filtering is:  10225\n",
      "Total length of short content dictionary after filtering is:  1671\n",
      "Total number of long articles is:  4894\n",
      "Total number of long articles after dropping blank ones:  3899\n",
      "Total number of unique long articles is:  1551\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509fe94bc6e24c2ca538c69a4770fc60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1551, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total length of long content dictionary before filtering is:  37538\n",
      "Total length of long content dictionary after filtering is:  7674\n",
      "AllContent2019-03-04.csv done! Next file:\n",
      "AllContent2019-03-05.csv\n",
      "Total number of short articles is:  2593\n",
      "Total number of short articles after dropping blank ones:  2007\n",
      "Total number of unique short articles is:  1764\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a089a527147143c1a61ac3ed443dff44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1764, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total length of short content dictionary before filtering is:  11198\n",
      "Total length of short content dictionary after filtering is:  1791\n",
      "Total number of long articles is:  2593\n",
      "Total number of long articles after dropping blank ones:  2025\n",
      "Total number of unique long articles is:  1753\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63869ed10b04d4aa557e900d9c67dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1753, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# directory = \"/home/sdbookhu/FullNews/\"\n",
    "\n",
    "directory = \"C:/Users/Shreya/Desktop/Threat_detective/all un news/\"\n",
    "\n",
    "for files in glob.glob(directory+'*.csv'):\n",
    "\n",
    "    print(files[53:]) #23\n",
    "\n",
    "#     save_directory = \"/home/shreyac/cleaned_news/\"\n",
    "\n",
    "    name = files[53:] # 23\n",
    "\n",
    "    news_csv_file = pd.read_csv(files)\n",
    "\n",
    "    data_all = news_csv_file\n",
    "    data_url = data_all['url']\n",
    "    data_source = data_all['source_name']\n",
    "    data_publish_time = data_all['publishedAt']\n",
    "    data_title = data_all['title']\n",
    "    data_description = data_all['description']\n",
    "    data_short_content = data_all['content']\n",
    "    data_full_content = data_all['full_content']\n",
    "    \n",
    "    # data_full_content = data_all['full-content']\n",
    "\n",
    "\n",
    "    # ------------------- PRE-PROCESS SHORT ARTICLES ---------------------------------------------------------\n",
    "\n",
    "\n",
    "    short_data_text = data_all[['content', 'title']]\n",
    "    print(\"Total number of short articles is: \", len(short_data_text))\n",
    "    short_data_text = short_data_text.dropna() # Not all articles have \"full content\" available\n",
    "    print(\"Total number of short articles after dropping blank ones: \", len(short_data_text))\n",
    "    short_data_text = short_data_text.drop_duplicates(subset=\"title\", keep = \"last\") # We have many repeating articles\n",
    "    print(\"Total number of unique short articles is: \", len(short_data_text))\n",
    "    \n",
    "    \n",
    "    \n",
    "    short_documents = short_data_text[['content']]\n",
    "    \n",
    "    short_remove_special_characters = re.compile('([^\\w\\s-]|_)+')\n",
    "    short_documents['content'].replace(to_replace= short_remove_special_characters, value='', regex=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    # We will apply preprocessing to the whole dataframe\n",
    "\n",
    "    short_processed_docs = short_documents['content'].swifter.apply(preprocess_text).to_frame(\"short_processed_text\")\n",
    "    \n",
    "    \"\"\"\n",
    "    To get full \"dictionary\" -- for word frequencies\n",
    "    \"\"\"\n",
    "\n",
    "    short_all_processed_docs_list = short_processed_docs.short_processed_text.to_list() # Converts all rows to one big list of lists\n",
    "    short_all_processed_docs_list = [item for sublist in short_all_processed_docs_list for item in sublist] # List of lists to one simple list\n",
    "\n",
    "    short_all_docs_frequency = get_frequency(short_all_processed_docs_list) # Using the function written above\n",
    "\n",
    "\n",
    "    # Vocabulary object:\n",
    "    # This takes in all unique words from each file of articles and appends to a list\n",
    "    # We also add these unique words to a text file\n",
    "\n",
    "    short_all_vocabulary = open(\"short_all_vocabulary.txt\", encoding=\"utf8\").readlines()\n",
    "    \n",
    "    short_all_vocabulary = set(short_all_vocabulary)\n",
    "\n",
    "    for word in short_all_docs_frequency:\n",
    "        if word[0] not in short_all_vocabulary:\n",
    "            short_all_vocabulary.append(word[0])\n",
    "\n",
    "    short_all_vocabulary.sort()\n",
    "\n",
    "    with open('short_all_vocabulary.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        for word in short_all_vocabulary:\n",
    "            f.write(\"%s\" % word)   \n",
    "\n",
    "\n",
    "    # NOTE: This includes all the numbers appearing in each article\n",
    "    \n",
    "    # Gensim's in-built dictionary\n",
    "\n",
    "    short_text_dictionary = gensim.corpora.Dictionary(short_processed_docs.short_processed_text)\n",
    "\n",
    "    \"\"\"\n",
    "    gensim has its own high and low pass filters as shown below.\n",
    "    However, we are unable to see exactly which words were removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Include words in dictionary that appear greater than 5 times - Low pass\n",
    "    # but less than 0.5 proportion of the frequency of all the words in all of the articles - High pass\n",
    "    print(\"Total length of short content dictionary before filtering is: \", len(short_text_dictionary))\n",
    "    short_text_dictionary.filter_extremes(no_below = 5, no_above=0.5) \n",
    "    print(\"Total length of short content dictionary after filtering is: \", len(short_text_dictionary))\n",
    "\n",
    "    # Blacklist object:\n",
    "    # This takes in all the \"words\" that were filtered out and appends to a list\n",
    "    # We also add these filtered out \"words\" to a text file\n",
    "    # We also create a separate object that maintains all \"words\" not filtered out and create a text file for the same\n",
    "\n",
    "    short_blacklist = open(\"short_blacklist.txt\", encoding=\"utf8\").readlines()\n",
    "    short_retained_vocabulary = open(\"short_retained_vocabulary.txt\", encoding=\"utf8\").readlines()\n",
    "    \n",
    "    short_blacklist = set(short_blacklist)\n",
    "    short_retained_vocabulary = set(short_retained_vocabulary)\n",
    "    \n",
    "\n",
    "    for word in short_all_vocabulary:\n",
    "        if word not in short_text_dictionary.token2id.keys():\n",
    "            short_blacklist.append(word)\n",
    "        else:\n",
    "            short_retained_vocabulary.append(word)\n",
    "\n",
    "    short_blacklist.sort()\n",
    "    short_retained_vocabulary.sort()\n",
    "\n",
    "    with open('short_blacklist.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        for word in short_blacklist:\n",
    "            f.write(\"%s\\n\" % word)\n",
    "\n",
    "    with open('short_retained_vocabulary.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        for word in short_retained_vocabulary:\n",
    "            f.write(\"%s\\n\" % word)\n",
    "\n",
    "\n",
    "    # NOTE: This includes all the numbers appearing in each article\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    All the words have been mapped to an \"ID\" using gensim.corpora.Dictionary\n",
    "    Now, within each individual \"document\" (news article), we can get the corresponding word counts.\n",
    "    Unfortunately, gensim only allows for getting document-level word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # TF-IDF depending on the time and filtering window\n",
    "\n",
    "    # short_main_corpus = [short_text_dictionary.doc2bow(doc) for doc in short_processed_docs]\n",
    "\n",
    "\n",
    "    # # TF-IDF on the bag of words corpus\n",
    "\n",
    "    # short_tfidf = models.TfidfModel(short_main_corpus)\n",
    "    # short_tfidf_main_corpus = tfidf[short_main_corpus]\n",
    "    \n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "    # ------------------- PRE-PROCESS LONG ARTICLES ---------------------------------------------------------\n",
    "\n",
    "    # For LONG (full) articles \n",
    "\n",
    "    long_data_text = data_all[['full_content', 'title']]\n",
    "    print(\"Total number of long articles is: \", len(long_data_text))\n",
    "    long_data_text = long_data_text.dropna() # Not all articles have \"full content\" available\n",
    "    print(\"Total number of long articles after dropping blank ones: \", len(long_data_text))\n",
    "    long_data_text = long_data_text.drop_duplicates(subset=\"title\", keep = \"last\") # We have many repeating articles\n",
    "    print(\"Total number of unique long articles is: \", len(long_data_text))\n",
    "\n",
    "    long_documents = long_data_text[['full_content']]\n",
    "\n",
    "    # Remove hyperlinks from content\n",
    "    long_link_remove = re.compile(r'http\\S+')                             \n",
    "    long_documents['full_content'].replace(to_replace= long_link_remove, value='', regex=True, inplace=True)\n",
    "\n",
    "    long_remove_let_your_friends_know = re.compile(r'Let friends in your social network know what you are reading about .*? Please read the rules before joining the discussion.')\n",
    "    long_documents['full_content'].replace(to_replace= long_remove_let_your_friends_know, value='', regex=True, inplace=True)\n",
    "\n",
    "    long_remove_last_for_more_coverage_1 = re.compile(r'___ For more .*? This material may not be published, broadcast, rewritten or redistributed.')\n",
    "    long_remove_last_for_more_coverage_2 = re.compile(r'___ For more .*? by Automated Insights,  using data from STATS LLC, ')\n",
    "    long_remove_last_for_more_coverage_3 = re.compile(r'For more AP.*? by Automated Insights,  using data from STATS LLC, ')\n",
    "    long_documents['full_content'].replace(to_replace= long_remove_last_for_more_coverage_1, value='', regex=True, inplace=True)\n",
    "    long_documents['full_content'].replace(to_replace= long_remove_last_for_more_coverage_2, value='', regex=True, inplace=True)\n",
    "    long_documents['full_content'].replace(to_replace= long_remove_last_for_more_coverage_3, value='', regex=True, inplace=True)\n",
    "\n",
    "\n",
    "    long_remove_special_characters = re.compile('([^\\w\\s-]|_)+')\n",
    "    long_documents['full_content'].replace(to_replace= long_remove_special_characters, value='', regex=True, inplace=True)\n",
    "\n",
    "    # We will apply preprocessing to the whole dataframe\n",
    "\n",
    "    long_processed_docs = long_documents['full_content'].swifter.apply(preprocess_text).to_frame(\"long_processed_text\")\n",
    "\n",
    "    \"\"\"\n",
    "    To get full \"dictionary\" -- for word frequencies\n",
    "    \"\"\"\n",
    "\n",
    "    long_all_processed_docs_list = long_processed_docs.long_processed_text.to_list() # Converts all rows to one big list of lists\n",
    "    long_all_processed_docs_list = [item for sublist in long_all_processed_docs_list for item in sublist] # List of lists to one simple list\n",
    "\n",
    "    long_all_docs_frequency = get_frequency(long_all_processed_docs_list) \n",
    "\n",
    "\n",
    "    # Vocabulary object:\n",
    "    # This takes in all unique words from each file of articles and appends to a list\n",
    "    # We also add these unique words to a text file\n",
    "\n",
    "    long_all_vocabulary = open(\"long_all_vocabulary.txt\", encoding=\"utf8\").readlines()\n",
    "    \n",
    "    long_all_vocabulary = set(long_all_vocabulary)\n",
    "\n",
    "    for word in long_all_docs_frequency:\n",
    "        if word[0] not in long_all_vocabulary:\n",
    "            long_all_vocabulary.append(word[0])\n",
    "\n",
    "    long_all_vocabulary.sort()\n",
    "\n",
    "    with open('long_all_vocabulary.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        for word in long_all_vocabulary:\n",
    "            f.write(\"%s\\n\" % word)   \n",
    "\n",
    "\n",
    "    # NOTE: This includes all the numbers appearing in each article\n",
    "\n",
    "\n",
    "    # Gensim's in-built dictionary\n",
    "\n",
    "    long_text_dictionary = gensim.corpora.Dictionary(long_processed_docs.long_processed_text)\n",
    "\n",
    "    \"\"\"\n",
    "    gensim has its own high and low pass filters as shown below.\n",
    "    However, we are unable to see exactly which words were removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Include words in dictionary that appear greater than 5 times - Low pass\n",
    "    # but less than 0.5 proportion of the frequency of all the words in all of the articles - High pass\n",
    "    print(\"Total length of long content dictionary before filtering is: \", len(long_text_dictionary))\n",
    "    long_text_dictionary.filter_extremes(no_below = 5, no_above=0.5) \n",
    "    print(\"Total length of long content dictionary after filtering is: \", len(long_text_dictionary))\n",
    "\n",
    "\n",
    "\n",
    "    # Blacklist object:\n",
    "    # This takes in all the \"words\" that were filtered out and appends to a list\n",
    "    # We also add these filtered out \"words\" to a text file\n",
    "    # We also create a separate object that maintains all \"words\" not filtered out and create a text file for the same\n",
    "\n",
    "    long_blacklist = open(\"long_blacklist.txt\", encoding=\"utf8\").readlines()\n",
    "    long_retained_vocabulary = open(\"long_retained_vocabulary.txt\", encoding=\"utf8\").readlines()\n",
    "    \n",
    "    long_blacklist = set(long_blacklist)\n",
    "    long_retained_vocabulary = set(long_retained_vocabulary)\n",
    "\n",
    "    for word in long_all_vocabulary:\n",
    "        if word not in long_text_dictionary.token2id.keys():\n",
    "            long_blacklist.append(word)\n",
    "        else:\n",
    "            long_retained_vocabulary.append(word)\n",
    "\n",
    "    long_blacklist.sort()\n",
    "    long_retained_vocabulary.sort()\n",
    "\n",
    "    with open('long_blacklist.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        for word in long_blacklist:\n",
    "            f.write(\"%s\\n\" % word)\n",
    "\n",
    "    with open('long_retained_vocabulary.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        for word in long_retained_vocabulary:\n",
    "            f.write(\"%s\\n\" % word)\n",
    "\n",
    "\n",
    "    # NOTE: This includes all the numbers appearing in each article\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    All the words have been mapped to an \"ID\" using gensim.corpora.Dictionary\n",
    "    Now, within each individual \"document\" (news article), we can get the corresponding word counts.\n",
    "    Unfortunately, gensim only allows for getting document-level word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # TF-IDF depending on the time and filtering window\n",
    "\n",
    "    # long_main_corpus = [long_text_dictionary.doc2bow(doc) for doc in long_processed_docs]\n",
    "\n",
    "\n",
    "    # # TF-IDF on the bag of words corpus\n",
    "\n",
    "    # long_tfidf = models.TfidfModel(long_main_corpus)\n",
    "    # long_tfidf_main_corpus = tfidf[long_main_corpus]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------------------------------\n",
    "    \n",
    "    print(\" \")\n",
    "    print(name + \" done! Next file:\")\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
