{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/schandrasekharan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd \n",
    "import re\n",
    "from pandas import Series, DataFrame\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "import gensim\n",
    "from gensim import models # For TF-IDF, LDA\n",
    "import swifter # Makes applying to datframe as fast as vectorizing\n",
    "import numpy as np\n",
    "import collections\n",
    "import csv\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# LDA Visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------- FUNCTIONS ---------------------------------------------------------\n",
    "\n",
    "# Pre-procesing function\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.extend(['chars', 'char']) # Add from blacklist\n",
    "\n",
    "stop_words.extend(['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'])\n",
    "\n",
    "# stop_words.extend(['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december'])\n",
    "# \n",
    "\n",
    "stop_words.extend(['get', 'say', 'gmt', 'dont', 'make', 'want', 'also', \n",
    "                   'take', 'since', 'tell', 'like', 'could', 'would', \n",
    "                   'should', 'jsfjsdgetelementsbytagnames0p', 'functiondsidvar']) # Adding from LDA topics \n",
    "\n",
    "def preprocess_text(doc):\n",
    "    \n",
    "    \"\"\"\n",
    "    Pre-processing using TextBlob: \n",
    "    tokenizing, converting to lower-case, and lemmatization based on POS tagging, \n",
    "    removing stop-words, and retaining tokens greater than length 2\n",
    "    \n",
    "    We also include n_grams (n = 1,2,3,4) in the final output\n",
    "    \n",
    "    Argument(s): 'doc' - a string of words or sentences.\n",
    "    \n",
    "    Output: 'reuslt' - a list of pre-processed tokens of each sentence in 'doc'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    blob = TextBlob(doc).lower() \n",
    "    \n",
    "    result = []\n",
    "    tag_dict = {\"J\": 'a', # Adjective\n",
    "                \"N\": 'n', # Noun\n",
    "                \"V\": 'v', # Verb\n",
    "                \"R\": 'r'} #  Adverb\n",
    "    \n",
    "    # For all other types of parts of speech (including those not classified at all) \n",
    "    # the tag_dict object maps to 'None'\n",
    "    # the method w.lemmatize() defaults to 'Noun' as POS for those classified as 'None'\n",
    "    \n",
    "    \n",
    "    bigrams = blob.ngrams(n = 2)\n",
    "    trigrams = blob.ngrams(n = 3)\n",
    "    fourgrams = blob.ngrams(n = 4)\n",
    "\n",
    "    for sent in blob.sentences:\n",
    "\n",
    "        words_and_tags = [(w, tag_dict.get(pos[0])) for w, pos in sent.tags]\n",
    "        lemmatized_list = [w.lemmatize(tag) for w, tag in words_and_tags]\n",
    "\n",
    "        for token in lemmatized_list:\n",
    "            if token not in stop_words and len(token.lower()) > 2:\n",
    "                result.append(token.lower())\n",
    "\n",
    "                \n",
    "    return result + ['_'.join(i) for i in bigrams] + ['_'.join(i) for i in trigrams] + ['_'.join(i) for i in fourgrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Natural Language Processing (NLP) is an area of growing attention due to increasing number of applications like chatbots, machine translation etc. In some ways, the entire revolution of intelligent machines in based on the ability to understand and interact with humans. I have been exploring NLP for some time now.  My journey started with NLTK library in Python, which was the recommended library to get started at that time. NLTK is a perfect library for education and research, it becomes very heavy and tedious for completing even the simple tasks.\"\n",
    "\n",
    "# preprocess_text(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for word frequencies\n",
    "\n",
    "def get_frequency(processed_text_list): \n",
    "\n",
    "    \"\"\"\n",
    "    Using a built-in NLTK function that generates tuples\n",
    "    We get the frequency distribution of all words/n-grams in a tokenized list\n",
    "    We also sort these frequencies in descending order in a dictionary object.\n",
    "    \n",
    "    Argument(s): 'processed_text_list' - A list of pre-processed tokens\n",
    "    \n",
    "    Output: sorted_counts - A dictionary of tokens and their respective counts in descending order\n",
    "    \"\"\"\n",
    "\n",
    "    word_frequency = FreqDist(word for word in processed_text_list)\n",
    "\n",
    "    sorted_counts = sorted(word_frequency.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    return dict(sorted_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_frequency(preprocess_text(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab_dictionary(vocab_column, name = 'test_file'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes any number of vocabulary frequency dictionaries \n",
    "    (here, all from 1 column) and merges them while summing \n",
    "    the respective frequencies and saves this merged dictionary\n",
    "    to a text file\n",
    "    \n",
    "    \n",
    "    Argument(s): vocab_column - A column of dictionary objects\n",
    "                 name (string object) - the name to be given to the text file\n",
    "    Output(s): a list object containing all the frequency dictionaries\n",
    "               a saved text file containing all the dictionary elements\n",
    "               a saved CSV file containing all the dictionary elements\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    merged_dict = {}\n",
    "    for dictionary in vocab_column:\n",
    "        for key, value in dictionary.items():  # d.items() in Python 3+\n",
    "#             merged_dict[key].append(value)\n",
    "            merged_dict.setdefault(key, []).append(value)\n",
    "\n",
    "\n",
    "    for key, value in merged_dict.items():\n",
    "        merged_dict[key] = sum(value)\n",
    "    \n",
    "    \n",
    "#     name = 'Apr_01_2019_window_1' \n",
    "    save_name_txt = name + '.txt'\n",
    "    save_name_csv = name + '.csv'\n",
    "    \n",
    "    with open(save_name_txt, 'w+', encoding = \"utf-8\") as file1:\n",
    "        for key, value in merged_dict.items():\n",
    "            file1.write(\"%s\\n\" % f\"{key}: {value}\")\n",
    "            \n",
    "\n",
    "    with open(save_name_csv, 'w+') as file2:\n",
    "        file2.write(\"%s,%s\\n\"%('Token', 'Frequency'))\n",
    "        for key, value in merged_dict.items():\n",
    "            file2.write(\"%s,%s\\n\"%(key, value)) # ----> No headers\n",
    "       \n",
    "#     with open(save_name_csv, mode = 'w', newline = '') as file2:\n",
    "#         writer = csv.DictWriter(save_name_csv, fieldnames = ['Token', 'Frequency'])\n",
    "#         writer.writeheader()\n",
    "#         for key, value in merged_dict.items():\n",
    "#             writer.writerow({key: value}) # ---> Throws an error.\n",
    "     \n",
    "    return merged_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_thing = merge_vocab_dictionary(data_text['short_total_frequencies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blacklist(processed_tokens_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Outputs the tokens that have been filtered out using the gensim dictionary\n",
    "    \n",
    "    Argument(s): 'processed_tokens_list' - a list of pre-processed tokens\n",
    "    Output: 'blacklist' - a list of blacklisted tokens from a given list\n",
    "    \"\"\"\n",
    "    \n",
    "    blacklist = []\n",
    "    for token in processed_tokens_list:\n",
    "        if token not in short_text_dictionary.token2id.keys():\n",
    "            blacklist.append(token)\n",
    "        \n",
    "    return blacklist\n",
    "\n",
    "\n",
    "def retained(processed_tokens_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Outputs the tokens that have been retained after filtration using the gensim dictionary\n",
    "    \n",
    "    Argument(s): 'processed_tokens_list' - a list of pre-processed tokens\n",
    "    Output: 'retained' - a list of retained tokens from a given list\n",
    "    \"\"\"\n",
    "    \n",
    "    retained = []\n",
    "    for token in processed_tokens_list:\n",
    "        if token in short_text_dictionary.token2id.keys():\n",
    "            retained.append(token)\n",
    "        \n",
    "    return retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = \"C:/Users/Shreya/Desktop/Threat_detective/all un news/\"\n",
    "\n",
    "# for files in glob.glob(directory + '*.csv'):\n",
    "\n",
    "#     print(files[53:]) #23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/Users/schandrasekharan/Desktop/Shreya_Personal/\"\n",
    "print(len(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = '/Users/schandrasekharan/Desktop/Shreya_Personal/'\n",
    "os.chdir(save_directory)\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1\n",
      "Total number of short and long articles is:  4872\n",
      "Total number of short and long articles after dropping blank ones:  3356\n",
      "Total number of unique short and long articles is:  1498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4042: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  method=method)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1 COMPLETE\n",
      "STEP 2\n",
      "STEP 2 COMPLETE\n",
      "STEP 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd1018a8877426bb806a90ccd65559c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762500f7d76848598f53a17473505483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3 COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# # ------------------- PRE-PROCESS ARTICLES ---------------------------------------------------------\n",
    "\n",
    "print(\"STEP 1\")\n",
    "\n",
    "data_all = pd.read_csv(\"/Users/schandrasekharan/Desktop/Shreya_Personal/AllContent2019-04-01.csv\")\n",
    "\n",
    "data_text = data_all.copy()\n",
    "\n",
    "data_text['old_index'] = data_text.index\n",
    "\n",
    "print(\"Total number of short and long articles is: \", len(data_text))\n",
    "\n",
    "data_text = data_text.dropna(subset = [\"content\", \"full-content\"]) # Not all articles have any content available\n",
    "print(\"Total number of short and long articles after dropping blank ones: \", len(data_text))\n",
    "\n",
    "data_text = data_text.drop_duplicates(subset=[\"title\", \"description\"], keep = 'last') # ---> Read below\n",
    "\n",
    "\"\"\"\n",
    "NOTE: The argument 'keep' changes the type of articles retained\n",
    "\n",
    "keep = 'last' ---> Ensures that the latest article is retained \n",
    "       (there might be changes in the content over time - as indicated by higher \n",
    "       retention of aritcle when 'url' is included in the subset.)\n",
    "       However, this leads to more AP sources\n",
    "       \n",
    "keep = 'first' ---> Retains original source more often.\n",
    "\n",
    "# We have many repeating articles\n",
    "# Dropping duplicates based on article title and description\n",
    "# Including \"source_id\" as a duplicate subset leads to fewer drops --> same article, different sources\n",
    "# Keeping least recent article (based on 'publishedAt')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Total number of unique short and long articles is: \", len(data_text))\n",
    "\n",
    "short_remove_special_characters = re.compile(r'([^\\w\\s-]|_)+')\n",
    "data_text[['content', 'full-content']].replace(to_replace= short_remove_special_characters, value=' ', regex=True, inplace=True)\n",
    "\n",
    "print(\"STEP 1 COMPLETE\")\n",
    "\n",
    "\n",
    "# For LONG (full) articles \n",
    "\n",
    "print(\"STEP 2\")\n",
    "\n",
    "# Remove hyperlinks from content\n",
    "long_link_remove = re.compile(r'http\\S+')                             \n",
    "long_remove_let_your_friends_know = re.compile(r'Let friends in your social network know what you are reading about .*? Please read the rules before joining the discussion.')\n",
    "long_remove_last_for_more_coverage_1 = re.compile(r'___ For more .*? This material may not be published, broadcast, rewritten or redistributed.')\n",
    "long_remove_last_for_more_coverage_2 = re.compile(r'___ For more .*? by Automated Insights,  using data from STATS LLC, ')\n",
    "long_remove_last_for_more_coverage_3 = re.compile(r'For more AP.*? by Automated Insights,  using data from STATS LLC, ')\n",
    "\n",
    "data_text['full-content'].replace(to_replace= [long_link_remove, long_remove_let_your_friends_know, long_remove_last_for_more_coverage_1, long_remove_last_for_more_coverage_2, long_remove_last_for_more_coverage_3], value='', regex=True, inplace=True)\n",
    "\n",
    "\n",
    "print(\"STEP 2 COMPLETE\")\n",
    "# data_text.head()\n",
    "\n",
    "\n",
    "# We will apply preprocessing to the whole dataframe \n",
    "\n",
    "# First Short\n",
    "\n",
    "print(\"STEP 3\")\n",
    "\n",
    "data_text['short_processed_text'] = data_text['content'].swifter.apply(preprocess_text)\n",
    "# Then Long\n",
    "data_text['long_processed_text'] = data_text['full-content'].swifter.apply(preprocess_text)\n",
    "\n",
    "print(\"STEP 3 COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c411aa5092634c18a3a190b3e0c3d8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0630f25aff44b4a080d4faa37dca62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 4 COMPLETE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"STEP 4\")\n",
    "\n",
    "data_text['short_all_frequencies'] = data_text['short_processed_text'].swifter.apply(get_frequency)\n",
    "data_text['long_all_frequencies'] = data_text['long_processed_text'].swifter.apply(get_frequency)\n",
    "    \n",
    "print(\"STEP 4 COMPLETE\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5\n",
      "STEP 5 COMPLETE\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 5\")\n",
    "\n",
    "short_full_vocab = merge_vocab_dictionary(data_text['short_all_frequencies'], name = 'short_Apr_01')\n",
    "long_full_vocab = merge_vocab_dictionary(data_text['long_all_frequencies'], name = 'long_Apr_01')\n",
    "\n",
    "print(\"STEP 5 COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UP TO THIS POINT IT IS GENERIC\n",
    "WE CAN DO THIS FOR EACH FULL_NEWS FILE AND MERGE \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6 \n",
      "Total length of short content dictionary before filtering is:  160184\n",
      "Total length of short content dictionary after filtering is:  3138\n",
      "Total length of long content dictionary before filtering is:  1589386\n",
      "Total length of long content dictionary after filtering is:  44609\n",
      "STEP 6 COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Gensim's in-built dictionary\n",
    "\n",
    "short_text_dictionary = gensim.corpora.Dictionary(data_text.short_processed_text)\n",
    "long_text_dictionary = gensim.corpora.Dictionary(data_text.long_processed_text)\n",
    "\n",
    "\"\"\"\n",
    "gensim has its own high and low pass filters as shown below.\n",
    "\"\"\"\n",
    "\n",
    "print(\"STEP 6 \")\n",
    "\n",
    "# Include words in dictionary that appear greater than 5 times - Low pass\n",
    "# but less than 0.4 proportion of the frequency of all the words in all of the articles - High pass\n",
    "print(\"Total length of short content dictionary before filtering is: \", len(short_text_dictionary))\n",
    "short_text_dictionary.filter_extremes(no_below = 5, no_above=0.4) \n",
    "print(\"Total length of short content dictionary after filtering is: \", len(short_text_dictionary))\n",
    "print(\"Total length of long content dictionary before filtering is: \", len(long_text_dictionary))\n",
    "long_text_dictionary.filter_extremes(no_below = 5, no_above=0.4) \n",
    "print(\"Total length of long content dictionary after filtering is: \", len(long_text_dictionary))\n",
    "\n",
    "print(\"STEP 6 COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b9b57eb2984716899b29cbfd94ed9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da868a93c7304938902b8e6019ec36ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e30ab438952466489e3b9538cfc9219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c2a402dec24fd391766133fc8c1049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_text['short_blacklist_vocab'] = data_text['short_processed_text'].swifter.apply(blacklist)\n",
    "data_text['long_blacklist_vocab'] = data_text['long_processed_text'].swifter.apply(blacklist)\n",
    "data_text['short_retained_vocab'] = data_text['short_processed_text'].swifter.apply(retained)\n",
    "data_text['long_retained_vocab'] = data_text['long_processed_text'].swifter.apply(retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3801530872874da8ac2fc3ef4128bd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6c205f0ffc4fb28b189259bcf917b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb8039c2d61428981ddd0ab0f6dc264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9e24c3f4394371b03acb7866a4d273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=1498, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_text['short_blacklist_frequencies'] = data_text['short_blacklist_vocab'].swifter.apply(get_frequency)\n",
    "data_text['long_blacklist_frequencies'] = data_text['long_blacklist_vocab'].swifter.apply(get_frequency)\n",
    "data_text['short_retained_frequencies'] = data_text['short_retained_vocab'].swifter.apply(get_frequency)\n",
    "data_text['long_retained_frequencies'] = data_text['long_retained_vocab'].swifter.apply(get_frequency)\n",
    "short_blacklist_vocab = merge_vocab_dictionary(data_text['short_blacklist_frequencies'], name = 'short_Apr_01_blacklist')\n",
    "long_blacklist_vocab = merge_vocab_dictionary(data_text['long_blacklist_frequencies'], name = 'long_Apr_01_blacklist')\n",
    "short_retained_vocab = merge_vocab_dictionary(data_text['short_retained_frequencies'], name = 'short_Apr_01_retained')\n",
    "long_retained_vocab = merge_vocab_dictionary(data_text['long_retained_frequencies'], name = 'long_Apr_01_retained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF depending on the time and filtering window\n",
    "\n",
    "# long_main_corpus = [long_text_dictionary.doc2bow(doc) for doc in long_processed_docs]\n",
    "\n",
    "\n",
    "# # TF-IDF on the bag of words corpus\n",
    "\n",
    "# long_tfidf = models.TfidfModel(long_main_corpus)\n",
    "# long_tfidf_main_corpus = tfidf[long_main_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##     --------------------------------------- SAVING PRE-PROCESSED TEXT TO FILE --------------------\n",
    "\n",
    "\n",
    "\n",
    "# print(\"STEP 17\")\n",
    "\n",
    "print(\"STEP 18\")\n",
    "\n",
    "data_url = data_text['url']\n",
    "data_source = data_text['source_name']\n",
    "data_publish_time = data_text['publishedAt']\n",
    "data_title = data_text['title']\n",
    "data_description = data_text['description']\n",
    "data_short_content = data_text['content']\n",
    "data_full_content = data_text['full-content']\n",
    "data_short_all_tokens = data_textx['short_processed_text']\n",
    "data_long_all_tokens = data_text['long_processed_text']\n",
    "data_short_all_frequencies = data_text['short_all_frequencies']\n",
    "data_long_all_frequencies = data_text['short_all_frequencies']\n",
    "data_short_blacklist_frequencies = data_text['short_blacklist_frequencies']\n",
    "data_long_blacklist_frequencies = data_text['short_blacklist_frequencies']\n",
    "data_short_retained_frequencies = data_text['short_all_frequencies']\n",
    "data_long_retained_frequencies = data_text['short_retained_frequencies']\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Use the script below to make changes to the CSV file and save as a different CSV file \n",
    "# \"\"\"\n",
    "\n",
    "# #     save_directory = \"/home/shreyac/cleaned_news/\"\n",
    "# save_directory = '/Users/schandrasekharan/Desktop/Shreya_Personal'\n",
    "\n",
    "# news_url = []\n",
    "# news_source = []\n",
    "# news_publish_time = []\n",
    "# news_title = []\n",
    "# news_description = []\n",
    "# news_short_content = []\n",
    "# news_full_content = []\n",
    "# short_processed_tokens = []\n",
    "# long_processed_tokens = []\n",
    "\n",
    "\n",
    "# for i in range(0, len(data_text_index)):\n",
    "\n",
    "#     news_url.append(data_url[i])\n",
    "#     news_source.append(data_source[i])\n",
    "#     news_publish_time.append(data_publish_time[i])\n",
    "#     news_title.append(data_title[i])\n",
    "#     news_description.append(data_description[i])\n",
    "#     news_short_content.append(data_short_content[i])\n",
    "#     news_full_content.append(data_full_content[i])\n",
    "#     short_processed_tokens.append(data_short_tokens[i])\n",
    "#     long_processed_tokens.append(data_long_tokens[i])\n",
    "\n",
    "\n",
    "# news_file_df = DataFrame({'url': news_url,\n",
    "#                 'source': news_source,\n",
    "#                 'published_at': news_publish_time,\n",
    "#                 'title': news_title,\n",
    "#                 'description': news_description,\n",
    "#                 'short_content': news_short_content,\n",
    "#                 'full_content': news_full_content,\n",
    "#                 'short_processed_tokens': short_processed_tokens,\n",
    "#                 'long_processed_tokens': long_processed_tokens})\n",
    "\n",
    "# news_file_df = news_file_df[['url', 'source', 'published_at', 'title',\n",
    "#                                  'description', 'short_content', 'full_content',\n",
    "#                              'short_processed_tokens', 'long_processed_tokens']]\n",
    "\n",
    "# save_path = save_directory + name + '.csv'\n",
    "\n",
    "# news_file_df.to_csv(save_path, index = None, header=True, encoding='utf-8')\n",
    "\n",
    "\n",
    "# print(\"STEP 19\")\n",
    "# print(\" \")\n",
    "# print(\"Loop complete\")\n",
    "# print(\"Next\")\n",
    "# print(\" \")\n",
    "# print(\" \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
