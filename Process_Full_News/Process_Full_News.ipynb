{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd \n",
    "import re\n",
    "from pandas import Series, DataFrame\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "import gensim\n",
    "from gensim import models # For TF-IDF, LDA\n",
    "import swifter # Makes applying to datframe as fast as vectorizing\n",
    "import numpy as np\n",
    "import collections\n",
    "import csv\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# LDA Visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening here:\n",
    "- Per time window\n",
    "    1. Save all vocab + frequencies\n",
    "    2. Save all blacklisted/retained vocab + frequencies ---> not done yet\n",
    "- Within each window\n",
    "    1. Pre-process:\n",
    "        - Clean (remove special characters, ect.)\n",
    "        - Remove stopwords\n",
    "        - Lemmatize based on POS\n",
    "        - Add n-grams\n",
    "        - Save tokens to new columns\n",
    "    2. Get vocab frequency for each article:\n",
    "        - Save token frequencies to new column\n",
    "    3. Merge all vocab tokens to get blacklist:\n",
    "        - Save all blacklisted and retained tokens in new text/csv files respectively\n",
    "        - Save all blacklisted and retained tokens in new columns respectively\n",
    "    4. Save new article CSV files with added columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------- FUNCTIONS ---------------------------------------------------------\n",
    "\n",
    "# Pre-procesing function\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.extend(['chars', 'char']) # Add from blacklist\n",
    "\n",
    "stop_words.extend(['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'])\n",
    "\n",
    "\n",
    "\n",
    "# stop_words.extend(['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december'])\n",
    "# ---> Not removing month names, some of them have different meanings \n",
    "\n",
    "stop_words.extend(['get', 'say', 'gmt', 'dont', 'make', 'want', 'also', \n",
    "                   'take', 'since', 'tell', 'like', 'could', 'would', 'etc',\n",
    "                   'should', 'jsfjsdgetelementsbytagnames0p', 'functiondsidvar']) # Adding from LDA topics \n",
    "\n",
    "def preprocess_text(doc, n_grams = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Pre-processing using TextBlob: \n",
    "    tokenizing, converting to lower-case, and lemmatization based on POS tagging, \n",
    "    removing stop-words, and retaining tokens greater than length 2\n",
    "    \n",
    "    We can also choose to include n_grams (n = 1,2,3,4) in the final output\n",
    "    \n",
    "    Argument(s): 'doc' - a string of words or sentences.\n",
    "                 'n_grams' - boolean: if set to 'True', n_grams are included\n",
    "    \n",
    "    Output: 'reuslt_singles' - a list of pre-processed tokens (individual words) of each sentence in 'doc'\n",
    "            'result_ngrams' - a list of pre-processed tokens (including n-grams) of each sentence in 'doc'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    doc = re.sub(r'([^\\w\\s-]|_)+', ' ',  doc) # ---> special characters\n",
    "    doc = re.sub(r'http\\S+', ' ', doc) # ---> hyperlinks\n",
    "    doc = re.sub(r'Let friends in your social network know what you are reading about .*? Please read the rules before joining the discussion.', ' ', doc) # ---> \n",
    "    doc = re.sub(r'___ For more .*? This material may not be published, broadcast, rewritten or redistributed.', ' ', doc) # --->\n",
    "    doc = re.sub(r'___ For more .*? by Automated Insights,  using data from STATS LLC, ', ' ', doc) # --->\n",
    "    doc = re.sub(r'For more AP.*? by Automated Insights,  using data from STATS LLC, ', ' ', doc) # --->\n",
    "\n",
    "    blob = TextBlob(doc).lower() \n",
    "    \n",
    "    result_singles = []\n",
    "    tag_dict = {\"J\": 'a', # Adjective\n",
    "                \"N\": 'n', # Noun\n",
    "                \"V\": 'v', # Verb\n",
    "                \"R\": 'r'} #  Adverb\n",
    "    \n",
    "    # For all other types of parts of speech (including those not classified at all) \n",
    "    # the tag_dict object maps to 'None'\n",
    "    # the method w.lemmatize() defaults to 'Noun' as POS for those classified as 'None'\n",
    "    \n",
    "    \n",
    "#     bigrams = blob.ngrams(n = 2)    # ---> not so useful, includes stopwords\n",
    "#     trigrams = blob.ngrams(n = 3)\n",
    "#     fourgrams = blob.ngrams(n = 4)\n",
    "\n",
    "    for sent in blob.sentences:\n",
    "\n",
    "        words_and_tags = [(w, tag_dict.get(pos[0])) for w, pos in sent.tags]\n",
    "        lemmatized_list = [w.lemmatize(tag) for w, tag in words_and_tags]\n",
    "\n",
    "        for i in range(len(lemmatized_list)):\n",
    "            if lemmatized_list[i] not in stop_words and len(lemmatized_list[i].lower()) > 2:\n",
    "                result_singles.append(lemmatized_list[i].lower())\n",
    "                \n",
    "    result_ngrams = result_singles + [' '.join(x) for x in ngrams(result_singles, 2)] + [' '.join(x) for x in ngrams(result_singles, 3)] + [' '.join(x) for x in ngrams(result_singles, 4)]\n",
    "\n",
    "#     + [' '.join(i) for i in bigrams] + [' '.join(i) for i in trigrams] + [''.join(i) for i in fourgrams]  # if we want to include stopwords          \n",
    "    \n",
    "    if n_grams == True:\n",
    "        result = result_ngrams\n",
    "    else:\n",
    "        result = result_singles\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc1 = \"Natural Language Processing (NLP) is an area of growing attention due to increasing number of applications like chatbots, machine translation etc. In some ways, the entire revolution of intelligent machines in based on the ability to understand and interact with humans. I have been exploring NLP for some time now.  My journey started with NLTK library in Python, which was the recommended library to get started at that time. NLTK is a perfect library for education and research, it becomes very heavy and tedious for completing even the simple tasks.\"\n",
    "# doc2 = \"This one contains multiple symbols.... like this: +327 -12, 63*... and this too %43 (jkfdjfdf). It just goes on to have more stuff $$ #\"\n",
    "# doc3 = \"html,https://static01.nyt.com/images/2019/03/28/realestate/02fix1/oakImage-1553807804790-facebookJumbo.jpg,2019-04-02T00:00:00Z,Alexa, Siri and Google Assistant can all make controlling a smart home with voice commands easy, but the idea of adding smart speakers with microphones that are always listening for the next command makes some people uneasy. Indeed, mistakes have been made. … [+1224 chars], Advertisement Supported by THE FIX You no longer need to renovate — or spend a lot of money — to incorporate smart technology into your home. Here’s how to do it in a few quick steps. By Tim McKeough Just a few years ago, creating a smart home with coordinated controls for lighting, motorized shades, multiroom audio, and heating and cooling required an expensive, comprehensive home-automation system and a renovation.  When you used to think about these systems, it was very much high-end,” said Mark Spates, a senior product manager for Nest and Google Home in Mountain View, Calif. “It was a luxury. Today, many of those functions are accessible through affordable, consumer-friendly products. You can pick and choose the functions you want and install many of the products yourself\"\n",
    "# a = preprocess_text(doc1)\n",
    "# b = preprocess_text(doc2)\n",
    "# c = preprocess_text(doc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for word frequencies\n",
    "\n",
    "def get_frequency(processed_text_list): \n",
    "\n",
    "    \"\"\"\n",
    "    Using a built-in NLTK function that generates tuples\n",
    "    We get the frequency distribution of all words/n-grams in a tokenized list\n",
    "    We get the proportion of the the token as a fraction of the total corpus size  ----> N/A\n",
    "    We also sort these frequencies and proportions in descending order in a dictionary object ----> N/A\n",
    "    \n",
    "    Argument(s): 'processed_text_list' - A list of pre-processed tokens\n",
    "    \n",
    "    Output(s): freq_dict - A dictionary of tokens and their respective frequencies in descending order\n",
    "    \"\"\"\n",
    "               #prop_dict - A dictionary of tokens and their respective proportions as a fraction of the total corpus\n",
    "               #combined_dict - A dictionary whose values are both frequencies and proportions combined within a list\n",
    "    #\"\"\"\n",
    "\n",
    "    word_frequency = FreqDist(word for word in processed_text_list)\n",
    "\n",
    "#     sorted_counts = sorted(word_frequency.items(), key = lambda x: x[1], reverse = True)\n",
    "#     freq_dict = dict(sorted_counts)\n",
    "    freq_dict = dict(word_frequency)\n",
    "#     prop_dict = {key : freq_dict[key] * 1.0 / sum(freq_dict.values()) for key, value in freq_dict.items()}\n",
    "#     combined_dict = {key : [freq_dict[key], freq_dict[key] * 1.0 / sum(freq_dict.values())] for key, value in freq_dict.items()}\n",
    "\n",
    "    return freq_dict #, prop_dict, combined_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab_dictionary(vocab_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes any number of token frequency dictionaries and merges them while summing \n",
    "    the respective frequencies and then calculates the proportion of the the tokens \n",
    "    as a fraction of the total corpus size and saves to text and CSV files\n",
    "    \n",
    "    \n",
    "    Argument(s): vocab_column - A column of dictionary objects\n",
    "                 \n",
    "    Output(s): merged_combined_dict - A list object containing the frequencies of all\n",
    "               merged dictionary tokens along with their respective proportions\n",
    "    \"\"\"\n",
    "    \n",
    "    merged_freq_dict = {}\n",
    "    for dictionary in vocab_column:\n",
    "        for key, value in dictionary.items():  # d.items() in Python 3+\n",
    "            merged_freq_dict.setdefault(key, []).append(value)\n",
    "\n",
    "\n",
    "    for key, value in merged_freq_dict.items():\n",
    "        merged_freq_dict[key] = sum(value)\n",
    "    \n",
    "    total_sum = sum(merged_freq_dict.values())\n",
    "#     merged_prop_dict = {key : merged_freq_dict[key] * 1.0 / total_sum for key, value in merged_freq_dict.items()}\n",
    "    merged_combined_dict = {key : [merged_freq_dict[key], (merged_freq_dict[key] * 1.0 / total_sum)] for key, value in merged_freq_dict.items()}\n",
    "     \n",
    "    return merged_combined_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blacklist(freq_prop_dict, name = 'test_file', less_than = 5, greater_than_prop = 0.4):\n",
    "    \n",
    "    \"\"\"\n",
    "    We use the \n",
    "    \n",
    "    Argument(s): 'freq_prop_dict' - a dictionary \n",
    "    \n",
    "    Output: 'blacklist' - a dictionary of blacklisted tokens from a given dictionary\n",
    "            'retained' - a dictionary of\n",
    "    \"\"\"\n",
    "\n",
    "    blacklist = {key : value for key, value in freq_prop_dict.items() if (value[0] < less_than or value[1] > greater_than_prop)}\n",
    "    retained = {key : value for key, value in freq_prop_dict.items() if (value[0] >= less_than and value[1] <= greater_than_prop)}\n",
    "        \n",
    "    return blacklist, retained\n",
    "\n",
    "# ---------- Other Approaches -----------\n",
    "\n",
    "#     blacklist = []\n",
    "#     retained = []\n",
    "#     for key, value in merged_dict.items():\n",
    "#     if value[0] < 5 and value[1] > 0.4:\n",
    "#         blacklist.append(key)\n",
    "#     else:\n",
    "#         retained.append(key)\n",
    "    \n",
    "    \n",
    "    \n",
    "#    # The gensim approach\n",
    "#    # Outputs the tokens that have been filtered out using the gensim dictionary\n",
    "#     for token in processed_tokens_list:\n",
    "#         if token not in short_text_dictionary.token2id.keys():\n",
    "#             blacklist.append(token)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_saver(save_dictionary, save_directory, save_name  = 'test_dictionary', text_file = True, csv_file = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    We use the \n",
    "    \n",
    "    Argument(s): 'freq_prop_dict' - a dictionary \n",
    "    \n",
    "    Output: a saved text file containing all the dictionary elements\n",
    "            a saved CSV file containing all the dictionary elements\n",
    "    \"\"\"\n",
    "    \n",
    "    if text_file == True:\n",
    "        \n",
    "        txt_save_name = save_directory + save_name +'.txt'\n",
    "    \n",
    "        with open(txt_save_name, 'w+', encoding = \"utf-8\") as txt_file:\n",
    "            for key, value in save_dictionary.items():\n",
    "                txt_file.write(\"%s\\n\" % f\"{key}: {value}\")\n",
    "                \n",
    "    if csv_file == True:\n",
    "        \n",
    "        csv_save_name = save_directory + save_name + '.csv'\n",
    "        \n",
    "        with open(csv_save_name, 'w+') as csv_file:\n",
    "            csv_file.write(\"%s,%s,%s\\n\"%('Token', 'Frequency', 'Proportion'))\n",
    "            for key, value in save_dictionary.items():\n",
    "                csv_file.write(\"%s,%s,%s\\n\"%(key, value[0], value[1]))\n",
    "                \n",
    "    return 'Files successfully saved'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_after_preprocess(processed_tokens, vocabulary_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    We use the \n",
    "    \n",
    "    Argument(s): processed_tokens  -\n",
    "                 vocabulary_dict -\n",
    "    \n",
    "    Output: filtered \n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "\n",
    "    for token in processed_tokens:\n",
    "        if token in vocabulary_dict.keys():\n",
    "            filtered.append(token)\n",
    "\n",
    "    return filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shall first import all the CSV files that we have on file\n",
    "# We store all these files in a list\n",
    "\n",
    "\n",
    "# directory = \"/home/sdbookhu/FullNews/\"\n",
    "\n",
    "rootdir_glob = '/home/sdbookhu/FullNews/**/*'\n",
    "\n",
    "all_articles = [f for f in glob.iglob('FullNews/**/*.csv', recursive=True) if os.path.isfile(f)]\n",
    "all_articles = sorted(all_articles, key = lambda file: file[-14:-4])\n",
    "\n",
    "print(\"The total number of CSV files is: \" + str(len(all_articles)))  \n",
    "\n",
    "# Now, we will read in files from the list based on our sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 2 # This is for a sliding window of size 1\n",
    "\n",
    "full_vocab_window_1 = {}\n",
    "blacklist_window_1 = {}\n",
    "retained_window_1 = {}\n",
    "\n",
    "main_vocab_list = [full_vocab_window_1, blacklist_window_1, retained_window_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, (len(all_articles) - window - 1)):\n",
    "\n",
    "for i in range(11, 20):\n",
    "    \n",
    "    df = pd.concat(map(pd.read_csv, all_articles[i:i+ window]), axis=0, ignore_index=True)\n",
    "    name = 'window_' + str(window) + all_articles[i][-14:-4]\n",
    "    print(name)\n",
    "    \n",
    "    path = '/home/sdbookhu/' + name + '/'\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    save_path = path\n",
    "    print(save_path)\n",
    "    # # ------------------- PRE-PROCESS ARTICLES ---------------------------------------------------------\n",
    "\n",
    "    print(\"STEP 1\")\n",
    "\n",
    "    data_text = df.copy() # Not altering the original\n",
    "\n",
    "    data_text['old_index'] = data_text.index\n",
    "\n",
    "    \"\"\"\n",
    "    We have this step so that we can locate which specific articles we have retained/deleted\n",
    "    The index gets altered when we remove duplicates and missing articles\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Total number of short and long articles is: \", len(data_text))\n",
    "\n",
    "    data_text = data_text.dropna(subset = [\"content\", \"full-content\"]) # Not all articles have any content available\n",
    "    print(\"Total number of short and long articles after dropping blank ones: \", len(data_text))\n",
    "\n",
    "    data_text = data_text.drop_duplicates(subset=[\"title\"], keep = 'last') # ---> Read below\n",
    "\n",
    "    \"\"\"\n",
    "    NOTE: The argument 'keep' changes the type of articles retained\n",
    "\n",
    "    keep = 'last' ---> Ensures that the latest article is retained \n",
    "           (there might be changes in the content over time - as indicated by higher \n",
    "           retention of aritcles when 'url' is included in the subset.)\n",
    "           However, this leads to more AP sources\n",
    "\n",
    "    keep = 'first' ---> Retains original source more often.\n",
    "\n",
    "    # We have many repeating articles\n",
    "    # Dropping duplicates based on article title and description\n",
    "    # Including \"source_id\" as a duplicate subset leads to fewer drops --> same article, different sources\n",
    "    # Keeping least recent article (based on 'publishedAt')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Total number of unique short and long articles is: \", len(data_text))\n",
    "\n",
    "    print(\"STEP 2\")\n",
    "    data_text['short_processed_text'] = data_text['content'].swifter.apply(preprocess_text)\n",
    "    # Then Long\n",
    "    data_text['long_processed_text'] = data_text['full-content'].swifter.apply(preprocess_text)\n",
    "\n",
    "    print(\"STEP 3\")\n",
    "\n",
    "    data_text['short_all_frequencies'] = data_text['short_processed_text'].swifter.apply(get_frequency)\n",
    "    data_text['long_all_frequencies'] = data_text['long_processed_text'].swifter.apply(get_frequency)\n",
    "\n",
    "    print(\"STEP 3 COMPLETE\")\n",
    "\n",
    "    print(\"STEP 4\")\n",
    "\n",
    "    short_full_vocab = merge_vocab_dictionary(data_text['short_all_frequencies'])\n",
    "    print(\"STEP 4 COMPLETE\")\n",
    "    print(\"STEP 5\")\n",
    "    long_full_vocab = merge_vocab_dictionary(data_text['long_all_frequencies'])\n",
    "\n",
    "    print(\"STEP 5 COMPLETE\")\n",
    "    print(\"STEP 6\")\n",
    "    short_blacklist_dict, short_retained_dict = blacklist(short_full_vocab)\n",
    "    print(\"STEP 7\")\n",
    "    long_blacklist_dict, long_retained_dict = blacklist(long_full_vocab)\n",
    "    print(\"STEP 7 COMPLETE\")\n",
    "    \n",
    "#     --------------------------------------------------------------\n",
    "\n",
    "    stop_words.extend(list(long_blacklist_dict.keys())) # ----> adding newly blacklisted words to stopwords\n",
    "    \n",
    "#     --------------------------------------------------------------\n",
    "\n",
    "    print(\"STEP 8\")\n",
    "    \n",
    "    save_directory = save_path\n",
    "    save_name = name + \"_short_full_vocab\"   \n",
    "    dictionary_saver(short_full_vocab, save_directory, save_name  = save_name, text_file = True, csv_file = True)\n",
    "    \n",
    "    save_name = name + \"_long_full_vocab\"   \n",
    "    dictionary_saver(long_full_vocab, save_directory, save_name  = save_name, text_file = True, csv_file = True)\n",
    "    \n",
    "    save_name = name + \"_short_blacklist\"   \n",
    "    dictionary_saver(short_blacklist_dict, save_directory, save_name  = save_name, text_file = True, csv_file = True)\n",
    "    \n",
    "    save_name = name + \"_long_blacklist\"   \n",
    "    dictionary_saver(long_blacklist_dict, save_directory, save_name  = save_name, text_file = True, csv_file = True)\n",
    "    \n",
    "    save_name = name + \"_short_retained\"   \n",
    "    dictionary_saver(short_retained_dict, save_directory, save_name  = save_name, text_file = True, csv_file = True)\n",
    "    \n",
    "    save_name = name + \"_long_retained\"   \n",
    "    dictionary_saver(long_retained_dict, save_directory,  save_name  = save_name, text_file = True, csv_file = True)\n",
    "    \n",
    "    print(\"STEP 8 COMPLETE\")\n",
    "    \n",
    "    print(\"STEP 9\")\n",
    "    \n",
    "    vocabulary_dict = short_retained_dict\n",
    "    data_text['short_retained']  = data_text['short_processed_text'].swifter.apply(filter_after_preprocess, args=(vocabulary_dict,))\n",
    "    \n",
    "    vocabulary_dict = long_retained_dict\n",
    "    data_text['long_retained']  = data_text['long_processed_text'].swifter.apply(filter_after_preprocess, args=(vocabulary_dict,))\n",
    "    \n",
    "    vocabulary_dict =  short_blacklist_dict\n",
    "    data_text['short_blacklist'] = data_text['long_processed_text'].swifter.apply(filter_after_preprocess, args=(vocabulary_dict,))\n",
    "    \n",
    "    vocabulary_dict = long_blacklist_dict\n",
    "    data_text['long_blacklist'] = data_text['long_processed_text'].swifter.apply(filter_after_preprocess, args=(vocabulary_dict,))\n",
    "    \n",
    "    print(\"STEP 9 COMPLETE\")\n",
    "    \n",
    "    print(\"STEP 10\")\n",
    "\n",
    "    data_old_index = data_text['old_index']\n",
    "    data_url = data_text['url']\n",
    "    data_source = data_text['source_name']\n",
    "    data_publish_time = data_text['publishedAt']\n",
    "    data_title = data_text['title']\n",
    "    data_description = data_text['description']\n",
    "    data_short_content = data_text['content']\n",
    "    data_full_content = data_text['full-content']\n",
    "    data_short_all_tokens = data_text['short_processed_text']\n",
    "    data_long_all_tokens = data_text['long_processed_text']\n",
    "    data_short_all_frequencies = data_text['short_all_frequencies']\n",
    "    data_long_all_frequencies = data_text['short_all_frequencies']\n",
    "    data_short_blacklist = data_text['short_blacklist']\n",
    "    data_long_blacklist = data_text['long_blacklist']\n",
    "    data_short_retained = data_text['short_retained']\n",
    "    data_long_retained = data_text['long_retained']\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Use the script below to make changes to the CSV file and save as a different CSV file \n",
    "    \"\"\"\n",
    "    \n",
    "    news_old_index = []\n",
    "    news_url = []\n",
    "    news_source = []\n",
    "    news_publish_time = []\n",
    "    news_title = []\n",
    "    news_description = []\n",
    "    news_short_content = []\n",
    "    news_full_content = []\n",
    "    short_all_processed_tokens = []\n",
    "    long_all_processed_tokens = []\n",
    "    short_all_frequencies = []\n",
    "    long_all_frequencies = []\n",
    "    short_blacklist = []\n",
    "    long_blacklist = []\n",
    "    short_retained = []\n",
    "    long_retained = []\n",
    "    \n",
    "\n",
    "    for i in range(0, len(data_text)):\n",
    "\n",
    "        news_old_index.append(data_old_index.iloc[i])\n",
    "        news_url.append(data_url.iloc[i])\n",
    "        news_source.append(data_source.iloc[i])\n",
    "        news_publish_time.append(data_publish_time.iloc[i])\n",
    "        news_title.append(data_title.iloc[i])\n",
    "        news_description.append(data_description.iloc[i])\n",
    "        news_short_content.append(data_short_content.iloc[i])\n",
    "        news_full_content.append(data_full_content.iloc[i])\n",
    "        short_all_processed_tokens.append(data_short_all_tokens.iloc[i])\n",
    "        long_all_processed_tokens.append(data_long_all_tokens.iloc[i])\n",
    "        short_all_frequencies.append(data_short_all_frequencies.iloc[i]) \n",
    "        long_all_frequencies.append(data_long_all_frequencies.iloc[i]) \n",
    "        short_retained.append(data_short_retained.iloc[i])\n",
    "        long_retained.append(data_long_retained.iloc[i])\n",
    "        short_blacklist.append(data_short_blacklist.iloc[i])\n",
    "        long_blacklist.append(data_long_blacklist.iloc[i])\n",
    "\n",
    "\n",
    "    news_file_df = DataFrame({\n",
    "                    'old_index': news_old_index,\n",
    "                    'url': news_url,\n",
    "                    'source': news_source,\n",
    "                    'publishedAt': news_publish_time,\n",
    "                    'title': news_title,\n",
    "                    'description': news_description,\n",
    "                    'content': news_short_content,\n",
    "                    'full-content': news_full_content,\n",
    "                    'short_all_tokens': short_all_processed_tokens,\n",
    "                    'long_all_tokens': long_all_processed_tokens,\n",
    "                    'short_all_frequencies':  short_all_frequencies,\n",
    "                    'long_all_frequencies':  long_all_frequencies,\n",
    "                    'short_retained_tokens': short_retained,\n",
    "                    'long_retained_tokens': long_retained,\n",
    "                    'short_blacklist_tokens': short_blacklist,\n",
    "                    'long_retained_blacklist': long_blacklist\n",
    "                    })\n",
    "\n",
    "    news_file_df = news_file_df[['old_index','url', 'source', 'publishedAt', 'title',\n",
    "                                 'description', 'content', 'full-content',\n",
    "                                 'short_all_tokens', 'long_all_tokens', 'short_all_frequencies',\n",
    "                                 'long_all_frequencies','short_retained_tokens', 'long_retained_tokens',\n",
    "                                 'short_blacklist_tokens', 'long_blacklist_tokens']]\n",
    "\n",
    "    save_directory = save_path + 'AllContent_' + name + '.csv'\n",
    "    \n",
    "    news_file_df.to_csv(save_directory, index = None, header=True, encoding='utf-8')\n",
    "    \n",
    "\n",
    "    print(\"STEP 10 COMPLETE\")\n",
    "    print(\" \")\n",
    "    print(\"Loop complete\")\n",
    "    print(\"Next\")\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving all vocabulary by window to main vocabulary file\n",
    "# # Can be done separately,  saves time.\n",
    "\n",
    "# full_vocab_window_1 = {}\n",
    "# blacklist_window_1 = {}\n",
    "# retained_window_1 = {}\n",
    "\n",
    "# main_vocab_list = [full_vocab_window_1, blacklist_window_1, retained_window_1] # --->  call from within files.\n",
    "\n",
    "# vocab_dicts = [short_full_vocab, long_full_vocab, short_blacklist, short_retained, long_blacklist, long_retained] # --->  call from within files.\n",
    "\n",
    "# for main_dict in main_vocab_list:\n",
    "#     for dictionary in vocab_dicts:\n",
    "#         for key, value in dictionary.items():  \n",
    "#             main_dict.setdefault(key, []).append(value)\n",
    "\n",
    "#     for key, value in main_dict.items():\n",
    "#         main_dict[key] = sum(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
